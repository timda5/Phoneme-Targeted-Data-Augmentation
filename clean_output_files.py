# clean_output_files.py
import re
import string
import collections
import os
from tqdm import tqdm

# --- Configuration ---
# Input files generated by oscar.py (using the "at least one" logic)
input_nl_file = 'analysis_outputs/output_nl_rich_sentences_turbo.txt'
input_fr_file = 'analysis_outputs/output_fr_rich_sentences_turbo.txt'

# Output files for the cleaned sentences
output_nl_cleaned_file = 'analysis_outputs/output_nl_atleast_one_cleaned_turbo.txt'
output_fr_cleaned_file = 'analysis_outputs/output_fr_atleast_one_cleaned_turbo.txt'

# --- Cleaning Parameters (Tune these as needed) ---
MIN_CHARS = 20
MAX_UPPER_RATIO = 0.30
MIN_ALPHA_RATIO = 0.70
MAX_DIGIT_RATIO = 0.10
MAX_PUNCT_RATIO = 0.15

# --- Keywords/Patterns to Filter Out (Keep or expand these) ---
# (Keep the BAD_KEYWORDS and BAD_PATTERNS lists as they were)
BAD_KEYWORDS = [
    # Web/UI
    'javascript', 'cookie', 'browser', 'copyright', 'http:', 'https:', '.nl', '.com', '.org', '.net', '.io', '.dev', '.app',
    'klik hier', 'lees meer', 'login', 'logout', 'password', 'gebruikersnaam', 'account', 'profiel', 'instellingen',
    'search', 'filter', 'submit', 'download', 'upload', 'privacy', 'terms', 'disclaimer', 'subscribe', 'unsubscribe',
    'newsletter', 'help', 'faq', 'contact', 'about us', 'home', 'next', 'previous', 'page', 'item', 'cart', 'checkout',
    'register', 'username', 'email address', 'share', 'tweet', 'like', 'follow', 'comment', 'reply',
    # Technical/Code/Markup
    'html', 'css', 'php', 'json', 'xml', 'api', 'sdk', 'url', 'uri', 'parameter', 'variable', 'function', 'class',
    'object', 'array', 'null', 'undefined', 'true', 'false', 'error code', 'status code', 'warning:', 'notice:', 'deprecated',
    '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.zip', '.rar', '.jpg', '.png', '.gif', '.svg', '.mp3', '.mp4', '.avi', '.log', '.yaml',
    # Placeholders/Metadata
    'lorem ipsum', 'placeholder', 'example', 'template', 'default', 'untitled', 'unknown', 'version', 'revision',
    'author', 'date', 'timestamp', 'id:', 'uuid', 'isbn', 'tags:', 'categorie:',
    # Ads/Tracking
    'ad', 'advertisement', 'sponsored', 'tracking', 'analytics',
    # Finance/Commerce
    'haccp', 'btw', '€', '$', 'usd', 'eur', 'price', 'discount', 'sale', 'order', 'shipping',
    # Common Warnings/Info
    'error', 'helaas', 'ondersteund', 'bericht', 'nieuws', 'update',
    # Licensing
    'cc by', 'cc-by', 'license', 'licentie', 'rights reserved',
    # Other common noise
    'menu', 'ingredienten', 'gerecht', 'foto', 'afbeelding', 'description:',
]

BAD_PATTERNS = re.compile(
    r'https?://\S+|'           # URLs
    r'www\.\S+|'              # URLs starting with www
    r'\S+@\S+\.\S+|'          # Email addresses
    r'\b[a-fA-F0-9]{10,}\b|'   # Long hex strings (>= 10 chars)
    r'(?:[a-zA-Z]:)?(?:[\\/][^\\/\n]+)+[\\/]?|' # Paths (improved slightly)
    r'\S+\.(?:com|nl|org|net|eu|be|io|dev|app|info|biz)\b/?\S*|' # Common TLDs
    r'\S*\.(?:txt|gz|js|css|php|html|xml|json|yaml|log|pdf|docx?|xlsx?|zip|rar|jpe?g|png|gif|svg|mp[34]|avi|mov|iso|img|exe|dll|so)\b|' # More file extensions
    r'\{[^{}]*\}|\[[^\[\]]*\]|<[^<>]*>|' # Things in brackets/tags
    r'(\d{1,3}\.){3}\d{1,3}|'   # IP addresses
    r'\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\b|' # Month names (often in logs/metadata)
    r'\b\d{1,2}[-/]\d{1,2}[-/]\d{2,4}\b|\b\d{4}[-/]\d{1,2}[-/]\d{1,2}\b|' # Date formats
    r'__\w+__|'               # Dunder methods/vars often in code examples
    r'\.{3,}|'                 # Ellipsis with 3 or more dots
    r'\b\d{4}\s?[A-Z]{2}\b|'   # Dutch postal codes
    r'(?:\+\d{1,3}[-.\s]?)?\(?\d{1,4}\)?[-.\s]?\d{1,4}[-.\s]?\d{1,9}\b|' # Phone numbers (basic)
    r'\b\d+[\.,]\d+\b|'       # Numbers with decimal points/commas
    r'^\s*[\d\W]+\s*$|'       # Lines containing mostly digits/non-word chars
    r'[\u2500-\u25FF]+|'       # Box drawing characters (potential ASCII art)
    r'(.)\1{5,}'              # Character repeated 6+ times
)
EXCESSIVE_CHARS = set('|»«•·{}[]<>\\^~*_')

# --- Cleaning Function ---
def is_clean_for_postprocessing(text: str) -> bool:
    """Enhanced cleaning function for post-processing."""
    if not text: return False
    char_count = len(text)
    if char_count < MIN_CHARS: return False

    if BAD_PATTERNS.search(text): return False

    lower_text = text.lower()
    if any(keyword in lower_text for keyword in BAD_KEYWORDS): return False

    alpha_count, upper_count, digit_count, punct_count = 0, 0, 0, 0
    excessive_char_found = False
    for char in text:
        if char.isalpha():
            alpha_count += 1
            if char.isupper(): upper_count += 1
        elif char.isdigit(): digit_count += 1
        elif char in string.punctuation: punct_count += 1
        if not excessive_char_found and char in EXCESSIVE_CHARS: excessive_char_found = True

    if excessive_char_found: return False
    if char_count == 0: return False
    if alpha_count / char_count < MIN_ALPHA_RATIO: return False
    if digit_count / char_count > MAX_DIGIT_RATIO: return False
    if punct_count / char_count > MAX_PUNCT_RATIO: return False
    if alpha_count > 0 and upper_count / alpha_count > MAX_UPPER_RATIO: return False

    words = text.split()
    if len(words) > 2 and all(word.isupper() for word in words if word.isalpha() and len(word)>1):
         return False

    counts = collections.Counter(text)
    if counts and counts.most_common(1)[0][1] / char_count > 0.6: return False

    return True

# --- File Processing Function (Modified for deduplication) ---
def clean_and_deduplicate_file(input_path, output_path):
    """Reads input, applies cleaning, removes ALL duplicates, writes to output."""
    if not os.path.exists(input_path):
        print(f"Input file not found: {input_path}")
        return 0, 0

    lines_read = 0
    unique_clean_sentences = set() # Use a set to store unique clean lines

    print(f"Cleaning and deduplicating {input_path} -> {output_path}...")

    try:
        # First pass to count lines for tqdm progress bar
        with open(input_path, 'r', encoding='utf-8') as infile_count:
            total_lines = sum(1 for _ in infile_count)

        # Second pass to clean and add to set
        with open(input_path, 'r', encoding='utf-8') as infile:
            for line in tqdm(infile, total=total_lines, desc=f"Reading {os.path.basename(input_path)}", unit=" lines"):
                lines_read += 1
                sentence = line.strip()
                if is_clean_for_postprocessing(sentence):
                    unique_clean_sentences.add(sentence) # Add to set (duplicates ignored)

        # Third pass: Write unique sentences from set to output file
        lines_written = len(unique_clean_sentences)
        print(f"Writing {lines_written} unique clean sentences to {output_path}...")
        with open(output_path, 'w', encoding='utf-8') as outfile:
            # Sort the set before writing for consistent order (optional)
            for sentence in tqdm(sorted(list(unique_clean_sentences)), desc=f"Writing {os.path.basename(output_path)}", unit=" lines"):
                outfile.write(sentence + '\n')

        read_percentage = (lines_written / lines_read * 100) if lines_read > 0 else 0
        print(f"Finished {input_path}. Read: {lines_read}, Unique Clean Written: {lines_written} ({read_percentage:.2f}%)")

    except Exception as e:
        print(f"Error processing file {input_path}: {e}")
        return lines_read, 0 # Return 0 written on error

    return lines_read, lines_written

# --- Main Execution ---
if __name__ == "__main__":
    print("Starting post-processing cleaning and deduplication...")
    clean_and_deduplicate_file(input_nl_file, output_nl_cleaned_file)
    clean_and_deduplicate_file(input_fr_file, output_fr_cleaned_file)
    print("\nPost-processing complete.")

