{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6507324-4041-48ea-81cf-dfcaedf29333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: __file__ not defined. Using current working directory.\n",
      "--- Starting Transcription for Group: PureDutchChildren_7_11_Large_2_Turbo ---\n",
      "Using device: cuda\n",
      "\n",
      "All required input files and directories found.\n",
      "Loaded 79 speaker codes from pure_dutch_children_7_11_codes.txt\n",
      "\n",
      "--- Loading Recording Metadata ---\n",
      "Successfully loaded recordings.txt with delimiter '\\t', encoding 'utf-8'.\n",
      "Successfully loaded recordings.txt with delimiter '\\t', encoding 'utf-8'.\n",
      "Combined 2 recording metadata file(s): 995 total entries.\n",
      "\n",
      "--- Filtering Recordings for Group: PureDutchChildren_7_11_Large_2_Turbo ---\n",
      "Filtering recordings for component: 'comp-q'\n",
      "Found 78 recordings matching speakers and component.\n",
      "\n",
      "--- Loading Whisper Model 'large-v2-turbo' ---\n",
      "Error loading Whisper model: Model large-v2-turbo not found; available models = ['tiny.en', 'tiny', 'base.en', 'base', 'small.en', 'small', 'medium.en', 'medium', 'large-v1', 'large-v2', 'large-v3', 'large', 'large-v3-turbo', 'turbo']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import whisper\n",
    "import glob\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import traceback\n",
    "import sys \n",
    "\n",
    "# --- Configuration ---\n",
    "IS_TEST_RUN = False\n",
    "NUM_FILES_TEST_RUN = 3\n",
    "\n",
    "# --- Paths ---\n",
    "try:\n",
    "    script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    print(\"Warning: __file__ not defined. Using current working directory.\")\n",
    "    script_dir = os.getcwd()\n",
    "\n",
    "GROUP_NAME = \"PureDutchChildren_7_11_Large_3_Turbo\"\n",
    "SPEAKER_CODES_FILE = os.path.join(script_dir, 'output_codes', 'pure_dutch_children_7_11_codes.txt')\n",
    "OUTPUT_DIR = os.path.join(script_dir, 'whisper_transcriptions', GROUP_NAME)\n",
    "\n",
    "# Base paths for JASMIN data relative to script location\n",
    "base_data_path = os.path.join(script_dir, \"jasmin-data/Data/data/meta/text\")\n",
    "audio_root_dir = os.path.join(script_dir, \"jasmin-data/Data/data/\") # Root containing 'audio' folder\n",
    "jasmin_ort_base_path = os.path.join(script_dir, \"jasmin-data/Data/data/annot/text/ort\") # Base path for finding .ort files\n",
    "\n",
    "# Input recording metadata paths\n",
    "nl_recordings_path = os.path.join(base_data_path, \"nl/recordings.txt\")\n",
    "vl_recordings_path = os.path.join(base_data_path, \"vl/recordings.txt\")\n",
    "\n",
    "# Model and Language\n",
    "WHISPER_MODEL_NAME = \"large-v3-turbo\" # e.g., \"base\", \"medium\", \"large-v3\"\n",
    "TRANSCRIPTION_LANGUAGE = \"nl\"   # Language code for transcription (e.g., \"nl\" for Dutch/Flemish)\n",
    "\n",
    "# Component Filter (Set to None or empty string \"\" to process all components)\n",
    "TARGET_COMPONENT = \"comp-q\" # e.g., \"comp-q\" or None\n",
    "\n",
    "REC_META_SPEAKER_COL = \"SpeakerID\" # Column name for speaker ID in recordings.txt\n",
    "REC_META_COMPONENT_COL = \"Component\"   # Column name for component in recordings.txt\n",
    "REC_META_FILEROOT_COL = \"Root\"   # Column name for the unique file identifier in recordings.txt\n",
    "\n",
    "# --- Data Loading Function (Robust Version - from previous script) ---\n",
    "def load_data_with_delimiters(file_path, potential_delimiters=['\\t', r'\\s+'], encoding='ISO-8859-1', expected_cols=None):\n",
    "    \"\"\"Attempts to load a CSV/text file using a list of potential delimiters.\"\"\"\n",
    "    last_exception = None\n",
    "    encodings_to_try = ['utf-8', encoding] # Try UTF-8 first\n",
    "\n",
    "    for enc in encodings_to_try:\n",
    "        for delim_raw in potential_delimiters:\n",
    "            delim_repr = repr(delim_raw)\n",
    "            try:\n",
    "                engine = 'python' if delim_raw == r'\\s+' else None\n",
    "                df = pd.read_csv(file_path, sep=delim_raw, encoding=enc,\n",
    "                                 engine=engine, on_bad_lines='warn', low_memory=False,\n",
    "                                 skipinitialspace=True, comment='#', skip_blank_lines=True)\n",
    "\n",
    "                if df.empty: continue\n",
    "                df.columns = df.columns.str.strip()\n",
    "\n",
    "                if expected_cols:\n",
    "                    missing_cols = [col for col in expected_cols if col not in df.columns]\n",
    "                    if not missing_cols:\n",
    "                        first_col_name = df.columns[0]\n",
    "                        if first_col_name and df[first_col_name].notna().any():\n",
    "                            print(f\"Successfully loaded {os.path.basename(file_path)} with delimiter {delim_repr}, encoding '{enc}'.\")\n",
    "                            return df\n",
    "                elif df.shape[1] > 1:\n",
    "                     first_col_name = df.columns[0]\n",
    "                     if first_col_name and df[first_col_name].notna().any():\n",
    "                         print(f\"Successfully loaded {os.path.basename(file_path)} with delimiter {delim_repr}, encoding '{enc}' ({df.shape[1]} columns found).\")\n",
    "                         return df\n",
    "            except pd.errors.ParserError as pe: last_exception = pe\n",
    "            except Exception as e: last_exception = e\n",
    "\n",
    "    print(f\"Error: Could not successfully load file {file_path} with any specified delimiter/encoding.\")\n",
    "    if last_exception: print(f\"Last error encountered: {last_exception}\")\n",
    "    return None\n",
    "\n",
    "# --- Helper Functions (Adapted from Whisper script) ---\n",
    "\n",
    "def load_speaker_codes(filepath):\n",
    "    \"\"\"Loads speaker codes from a text file (one code per line).\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Error: Speaker codes file not found at '{filepath}'\")\n",
    "        return []\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            codes = [line.strip() for line in f if line.strip()]\n",
    "        print(f\"Loaded {len(codes)} speaker codes from {os.path.basename(filepath)}\")\n",
    "        return codes\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading speaker codes file '{filepath}': {e}\")\n",
    "        return []\n",
    "\n",
    "def find_ort_file(speaker_code, component, file_root, config):\n",
    "    \"\"\"\n",
    "    Finds the corresponding .ort file path based on known JASMIN structure.\n",
    "    Args: speaker_code, component, file_root, config dict with JASMIN_ORT_BASE_PATH.\n",
    "    Returns: Full path to .ort file or None.\n",
    "    \"\"\"\n",
    "    if not all([speaker_code, component, file_root]):\n",
    "        print(f\"  Warning: Missing info for .ort lookup (Speaker: {speaker_code}, Comp: {component}, Root: {file_root}).\")\n",
    "        return None\n",
    "    ort_filename = f\"{file_root}.ort\"\n",
    "    region = 'nl' if speaker_code.startswith('N') else 'vl' if speaker_code.startswith('V') else None\n",
    "    if not region:\n",
    "        print(f\"  Warning: Cannot determine region from speaker code '{speaker_code}' for .ort lookup.\")\n",
    "        return None\n",
    "    ort_path = os.path.join(config['JASMIN_ORT_BASE_PATH'], component, region, ort_filename)\n",
    "    ort_path = os.path.normpath(ort_path)\n",
    "    return ort_path if os.path.exists(ort_path) else None\n",
    "\n",
    "def parse_ort_file(ort_path):\n",
    "    \"\"\" Parses a .ort file (potentially Praat TextGrid), trying multiple encodings. \"\"\"\n",
    "    if not ort_path or not os.path.exists(ort_path): return None\n",
    "    encodings_to_try = ['utf-8', 'latin1', 'iso-8859-1', 'cp1252']\n",
    "    content = None; detected_encoding = None\n",
    "    for encoding in encodings_to_try:\n",
    "        try:\n",
    "            with open(ort_path, 'r', encoding=encoding) as f: content = f.read()\n",
    "            detected_encoding = encoding; break\n",
    "        except UnicodeDecodeError: continue\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Error reading .ort file {ort_path} with encoding {encoding}: {e}\")\n",
    "            if not isinstance(e, UnicodeDecodeError): return None\n",
    "    if content is None:\n",
    "        print(f\"  Warning: Could not read .ort file '{os.path.basename(ort_path)}' with tried encodings.\")\n",
    "        return None\n",
    "\n",
    "    segments = []; lines = content.strip().split('\\n')\n",
    "    try:\n",
    "        # --- Strategy 1: Parse as Praat TextGrid ---\n",
    "        if len(lines) > 1 and (\"TextGrid\" in lines[1] or \"ooTextFile\" in lines[0]):\n",
    "            in_item = False; is_interval_tier = False; current_segment = {}\n",
    "            tier_name = None\n",
    "            # Find the relevant tier (e.g., 'ORT-MAU', 'ORT', 'transcript') - case insensitive\n",
    "            tier_found = False\n",
    "            target_tier_names = ['ort-mau', 'ort', 'transcript'] # Add other potential names\n",
    "            for i, line in enumerate(lines):\n",
    "                 line_lower = line.strip().lower()\n",
    "                 if line_lower.startswith(\"item [\"): tier_found = False # Reset for new item\n",
    "                 if line_lower.startswith(\"name =\"):\n",
    "                     current_tier_name_match = re.search(r'\"(.*)\"', line.strip())\n",
    "                     if current_tier_name_match:\n",
    "                         current_tier_name = current_tier_name_match.group(1).lower()\n",
    "                         if current_tier_name in target_tier_names:\n",
    "                             tier_name = current_tier_name\n",
    "                             tier_found = True\n",
    "                 if tier_found and \"IntervalTier\" in line: is_interval_tier = True; continue\n",
    "                 if tier_found and is_interval_tier and line.strip().startswith(\"item [\"): in_item = True; current_segment = {}; continue\n",
    "                 if in_item and is_interval_tier:\n",
    "                    line_strip = line.strip()\n",
    "                    if line_strip.startswith(\"xmin =\"): current_segment['start'] = float(line_strip.split('=')[1].strip())\n",
    "                    elif line_strip.startswith(\"xmax =\"): current_segment['end'] = float(line_strip.split('=')[1].strip())\n",
    "                    elif line_strip.startswith(\"text =\"):\n",
    "                        text_match = re.search(r'\"(.*)\"', line_strip)\n",
    "                        text = text_match.group(1).strip() if text_match else \"\"\n",
    "                        current_segment['text'] = text\n",
    "                        if 'start' in current_segment and 'end' in current_segment and 'text' in current_segment:\n",
    "                            # Only add segments with actual text content\n",
    "                            if current_segment['text'] and not current_segment['text'].isspace():\n",
    "                                segments.append(current_segment)\n",
    "                            in_item = False # Reset for next interval in the tier\n",
    "            if segments: print(f\"  Parsed {len(segments)} segments from TextGrid tier '{tier_name}'.\")\n",
    "\n",
    "        # --- Strategy 2: Simple format (Timestamp, Timestamp, \"Text\") ---\n",
    "        if not segments:\n",
    "            i = 0\n",
    "            while i < len(lines):\n",
    "                try:\n",
    "                    if i + 2 < len(lines):\n",
    "                        start_time = float(lines[i].strip())\n",
    "                        end_time = float(lines[i+1].strip())\n",
    "                        text_match = re.match(r'\\s*\"(.*)\"\\s*', lines[i+2])\n",
    "                        if text_match:\n",
    "                            text = text_match.group(1).strip()\n",
    "                            if text and not text.isspace(): segments.append({'start': start_time, 'end': end_time, 'text': text})\n",
    "                            i += 3; continue\n",
    "                except (ValueError, IndexError): pass\n",
    "                i += 1\n",
    "            if segments: print(f\"  Parsed {len(segments)} segments using simple timestamp format.\")\n",
    "\n",
    "        if segments: return segments\n",
    "        else:\n",
    "            print(f\"  Warning: Could not parse segments from '{os.path.basename(ort_path)}' (encoding: {detected_encoding}).\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"  Error during parsing .ort file {ort_path}: {e}\"); traceback.print_exc(); return None\n",
    "\n",
    "def format_whisper_segments(result):\n",
    "    \"\"\"Converts Whisper result segments into a standardized list of dictionaries.\"\"\"\n",
    "    segments_out = []\n",
    "    for seg in result.get('segments', []):\n",
    "        segments_out.append({\n",
    "            'start': seg.get('start'), 'end': seg.get('end'),\n",
    "            'text': seg.get('text', '').strip(), 'words': seg.get('words', [])\n",
    "        })\n",
    "    return segments_out\n",
    "\n",
    "def clean_text_for_wer(text):\n",
    "    \"\"\"Basic text cleaning for WER calculation (uppercase, remove punctuation).\"\"\"\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = text.upper()\n",
    "    text = re.sub(r'[^\\w\\s*]', '', text) # Keep '*' for potential markers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def prepare_wer_data(whisper_segments, ort_segments):\n",
    "    \"\"\"Prepares reference and hypothesis strings for WER analysis file.\"\"\"\n",
    "    if not ort_segments or not whisper_segments: return []\n",
    "    ref_full = \" \".join([seg['text'] for seg in ort_segments if seg.get('text')])\n",
    "    hyp_full = \" \".join([seg['text'] for seg in whisper_segments if seg.get('text')])\n",
    "    ref_clean = clean_text_for_wer(ref_full)\n",
    "    hyp_clean = clean_text_for_wer(hyp_full)\n",
    "    ops = \"?\"; csid = \"0 0 0 0\" # Placeholders, real WER tool needed\n",
    "    return [{'ref': ref_clean, 'hyp': hyp_clean, 'op': ops, 'csid': csid}]\n",
    "\n",
    "def save_results(output_dir, file_base_name, data_dict):\n",
    "    \"\"\"Saves detailed JSON and simple TXT results.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True) # Ensure dir exists\n",
    "    json_path = os.path.join(output_dir, f\"{file_base_name}.json\")\n",
    "    txt_path = os.path.join(output_dir, f\"{file_base_name}.txt\")\n",
    "    try:\n",
    "        with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            # Convert numpy types for JSON serialization\n",
    "            def convert_numpy(obj):\n",
    "                if isinstance(obj, np.integer): return int(obj)\n",
    "                elif isinstance(obj, np.floating): return float(obj)\n",
    "                elif isinstance(obj, np.ndarray): return obj.tolist()\n",
    "                return obj\n",
    "            json.dump(data_dict, f, indent=2, ensure_ascii=False, default=convert_numpy)\n",
    "    except Exception as e: print(f\"  Error saving JSON to {json_path}: {e}\")\n",
    "    try:\n",
    "        with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(data_dict.get('transcription', ''))\n",
    "    except Exception as e: print(f\"  Error saving TXT to {txt_path}: {e}\")\n",
    "\n",
    "def generate_wer_analysis_file(results_list, output_dir, group_name):\n",
    "    \"\"\"Generates a text file suitable for input to WER calculation tools.\"\"\"\n",
    "    wer_output_path = os.path.join(output_dir, f\"wer_analysis_{group_name}.txt\")\n",
    "    entries_written = 0\n",
    "    valid_results = [r for r in results_list if r.get('reference_parsed', False)]\n",
    "    if not valid_results:\n",
    "        print(f\"No results with parsed references found for group '{group_name}', skipping WER analysis file.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Generating WER analysis file for '{group_name}': {wer_output_path}\")\n",
    "    try:\n",
    "        with open(wer_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for result in valid_results:\n",
    "                file_id = f\"{result['speaker_code']}_{result['component']}_{result['file_root']}\"\n",
    "                json_file = os.path.join(output_dir, f\"{file_id}.json\")\n",
    "                wer_data_list = []\n",
    "                if os.path.exists(json_file):\n",
    "                    try:\n",
    "                        with open(json_file, \"r\", encoding=\"utf-8\") as jf:\n",
    "                             detailed_data = json.load(jf)\n",
    "                             wer_data_list = detailed_data.get('wer_data', [])\n",
    "                    except Exception as e: print(f\"  Warn: Could not reload JSON {json_file} for WER output: {e}\")\n",
    "\n",
    "                if wer_data_list:\n",
    "                     for wer_item in wer_data_list:\n",
    "                         f.write(f\"{file_id} ref {wer_item.get('ref','')}\\n\")\n",
    "                         f.write(f\"{file_id} hyp {wer_item.get('hyp','')}\\n\")\n",
    "                         f.write(f\"{file_id} op  {wer_item.get('op','?')}\\n\")\n",
    "                         f.write(f\"{file_id} #csid {wer_item.get('csid','0 0 0 0')}\\n\")\n",
    "                         entries_written += 1\n",
    "                else: print(f\"  Warn: No WER data found in {json_file} for {file_id}\")\n",
    "        if entries_written > 0: print(f\"WER analysis file generated with {entries_written // 4} entries.\")\n",
    "        else: print(f\"WER analysis file generation attempted, but no valid entries found/written.\")\n",
    "    except Exception as e: print(f\"Error generating WER analysis file: {e}\")\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "def main():\n",
    "    config = {\n",
    "        'IS_TEST_RUN': IS_TEST_RUN,\n",
    "        'NUM_FILES_TEST_RUN': NUM_FILES_TEST_RUN,\n",
    "        'OUTPUT_DIR': OUTPUT_DIR, # Specific output dir for the group\n",
    "        'WHISPER_MODEL_NAME': WHISPER_MODEL_NAME,\n",
    "        'TRANSCRIPTION_LANGUAGE': TRANSCRIPTION_LANGUAGE,\n",
    "        'JASMIN_ORT_BASE_PATH': jasmin_ort_base_path,\n",
    "        'AUDIO_ROOT_DIR': audio_root_dir,\n",
    "        'TARGET_COMPONENT': TARGET_COMPONENT,\n",
    "        'GROUP_NAME': GROUP_NAME, # Add group name to config\n",
    "    }\n",
    "\n",
    "    # --- Setup ---\n",
    "    os.makedirs(config['OUTPUT_DIR'], exist_ok=True)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"--- Starting Transcription for Group: {config['GROUP_NAME']} ---\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    overall_start_time = time.time()\n",
    "\n",
    "    # --- Check Input Paths ---\n",
    "    input_paths_to_check = [nl_recordings_path, vl_recordings_path, SPEAKER_CODES_FILE]\n",
    "    missing_paths = [p for p in input_paths_to_check if not os.path.exists(p)]\n",
    "    if missing_paths:\n",
    "        print(\"\\nError: Required input files not found:\")\n",
    "        for p in missing_paths: print(f\"- {os.path.abspath(p)}\")\n",
    "        sys.exit(1)\n",
    "    if not os.path.isdir(config['AUDIO_ROOT_DIR']):\n",
    "        print(f\"\\nError: Audio root directory not found: {os.path.abspath(config['AUDIO_ROOT_DIR'])}\")\n",
    "        sys.exit(1)\n",
    "    print(\"\\nAll required input files and directories found.\")\n",
    "\n",
    "    # --- Load Speaker Codes for the target group ---\n",
    "    target_speaker_codes = load_speaker_codes(SPEAKER_CODES_FILE)\n",
    "    if not target_speaker_codes:\n",
    "        print(f\"No speaker codes loaded for group {config['GROUP_NAME']}. Exiting.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- Load Recordings Metadata ---\n",
    "    print(\"\\n--- Loading Recording Metadata ---\")\n",
    "    expected_rec_cols = [REC_META_SPEAKER_COL, REC_META_COMPONENT_COL, REC_META_FILEROOT_COL]\n",
    "    nl_recordings = load_data_with_delimiters(nl_recordings_path, expected_cols=expected_rec_cols)\n",
    "    vl_recordings = load_data_with_delimiters(vl_recordings_path, expected_cols=expected_rec_cols)\n",
    "    all_recordings_df = None\n",
    "    loaded_rec_dfs = []\n",
    "    if nl_recordings is not None: loaded_rec_dfs.append(nl_recordings)\n",
    "    if vl_recordings is not None: loaded_rec_dfs.append(vl_recordings)\n",
    "    if loaded_rec_dfs:\n",
    "        all_recordings_df = pd.concat(loaded_rec_dfs, ignore_index=True)\n",
    "        print(f\"Combined {len(loaded_rec_dfs)} recording metadata file(s): {len(all_recordings_df)} total entries.\")\n",
    "    else:\n",
    "        print(\"\\nError: Failed to load any recording metadata. Exiting.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- Filter Recordings Metadata for Target Group ---\n",
    "    print(f\"\\n--- Filtering Recordings for Group: {config['GROUP_NAME']} ---\")\n",
    "    speaker_codes_set = set(target_speaker_codes)\n",
    "    group_recordings_df = all_recordings_df[all_recordings_df[REC_META_SPEAKER_COL].isin(speaker_codes_set)].copy()\n",
    "\n",
    "    # Optional: Filter by component\n",
    "    if config['TARGET_COMPONENT']:\n",
    "        print(f\"Filtering recordings for component: '{config['TARGET_COMPONENT']}'\")\n",
    "        group_recordings_df = group_recordings_df[group_recordings_df[REC_META_COMPONENT_COL] == config['TARGET_COMPONENT']].copy()\n",
    "        print(f\"Found {len(group_recordings_df)} recordings matching speakers and component.\")\n",
    "    else:\n",
    "        print(f\"Found {len(group_recordings_df)} recordings matching speakers (all components).\")\n",
    "\n",
    "    if group_recordings_df.empty:\n",
    "        print(f\"No recordings found for group {config['GROUP_NAME']} after filtering. Exiting.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- Load Whisper Model ---\n",
    "    print(f\"\\n--- Loading Whisper Model '{config['WHISPER_MODEL_NAME']}' ---\")\n",
    "    try:\n",
    "        model = whisper.load_model(config['WHISPER_MODEL_NAME'], device=device)\n",
    "        print(\"Model loaded.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Whisper model: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- Select files for processing (Test Run or All) ---\n",
    "    if config['IS_TEST_RUN']:\n",
    "        recordings_to_process = group_recordings_df.head(config['NUM_FILES_TEST_RUN'])\n",
    "        print(f\"--- TEST RUN: Processing first {len(recordings_to_process)} recordings for {config['GROUP_NAME']} ---\")\n",
    "    else:\n",
    "        recordings_to_process = group_recordings_df\n",
    "        print(f\"--- Processing all {len(recordings_to_process)} recordings for {config['GROUP_NAME']} ---\")\n",
    "\n",
    "    # --- Process Recordings ---\n",
    "    results_summary = []\n",
    "    start_time_group = time.time()\n",
    "    processed_count = 0\n",
    "    skipped_count = 0\n",
    "    ort_found_parsed_count = 0\n",
    "    ort_not_found_count = 0\n",
    "    ort_found_not_parsed_count = 0\n",
    "\n",
    "    for index, row in tqdm(recordings_to_process.iterrows(), total=len(recordings_to_process), desc=f\"Transcribing {config['GROUP_NAME']}\"):\n",
    "        try:\n",
    "            # Extract info from recordings metadata row\n",
    "            speaker_code = str(row[REC_META_SPEAKER_COL])\n",
    "            file_root = str(row[REC_META_FILEROOT_COL])\n",
    "            component = str(row[REC_META_COMPONENT_COL])\n",
    "            region = 'nl' if speaker_code.startswith('N') else 'vl' if speaker_code.startswith('V') else None\n",
    "\n",
    "            if not region:\n",
    "                print(f\"  Warning: Cannot determine region for SpeakerID '{speaker_code}'. Skipping.\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "\n",
    "            # Construct original audio path\n",
    "            wav_path = os.path.join(config['AUDIO_ROOT_DIR'], 'audio', 'wav', component, region, f\"{file_root}.wav\")\n",
    "            wav_path = os.path.normpath(wav_path)\n",
    "            file_name = os.path.basename(wav_path)\n",
    "            file_base_name = f\"{speaker_code}_{component}_{file_root}\"\n",
    "\n",
    "            print(f\"\\nProcessing: {file_name} (Speaker: {speaker_code}, Comp: {component})\")\n",
    "\n",
    "            # Check if audio file exists\n",
    "            if not os.path.exists(wav_path):\n",
    "                print(f\"  CRITICAL: Audio file not found at '{wav_path}'. Skipping.\")\n",
    "                fallback_pattern = os.path.join(config['AUDIO_ROOT_DIR'], 'audio', 'wav', component, region, f\"{file_root}*.wav\")\n",
    "                matching_files = glob.glob(fallback_pattern)\n",
    "                if matching_files:\n",
    "                    print(f\"  INFO: Found alternative audio file via fallback: {os.path.basename(matching_files[0])}\")\n",
    "                    wav_path = matching_files[0] # Use the first match\n",
    "                    file_name = os.path.basename(wav_path) \n",
    "                else:\n",
    "                    skipped_count += 1\n",
    "                    continue \n",
    "\n",
    "            # Find and Parse Reference (.ort) File\n",
    "            ort_path = find_ort_file(speaker_code, component, file_root, config)\n",
    "            ort_segments = parse_ort_file(ort_path) if ort_path else None\n",
    "            reference_parsed = ort_segments is not None\n",
    "            reference_found_path = ort_path if ort_path else \"\"\n",
    "\n",
    "            if reference_parsed:\n",
    "                print(f\"  Reference found and parsed: {os.path.basename(ort_path)}\")\n",
    "                ort_found_parsed_count +=1\n",
    "            elif ort_path:\n",
    "                print(f\"  Reference file found ({os.path.basename(ort_path)}) but FAILED TO PARSE.\")\n",
    "                ort_found_not_parsed_count += 1\n",
    "            else:\n",
    "                print(f\"  Reference .ort file NOT FOUND.\")\n",
    "                ort_not_found_count += 1\n",
    "\n",
    "            # Transcribe with Whisper\n",
    "            try:\n",
    "                result = model.transcribe(\n",
    "                    wav_path, language=config['TRANSCRIPTION_LANGUAGE'],\n",
    "                    word_timestamps=True, verbose=False # Set verbose=True temporarily for more Whisper output\n",
    "                )\n",
    "                transcription = result[\"text\"]\n",
    "                whisper_segments = format_whisper_segments(result)\n",
    "                print(f\"  Transcription finished.\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ERROR during Whisper transcription: {e}\")\n",
    "    \n",
    "                import traceback\n",
    "                traceback.print_exc() \n",
    "                skipped_count += 1\n",
    "                continue\n",
    "\n",
    "            # Prepare Data for WER analysis file\n",
    "            wer_data = prepare_wer_data(whisper_segments, ort_segments if reference_parsed else [])\n",
    "\n",
    "            # Save detailed JSON and simple TXT\n",
    "            output_data = {\n",
    "                'original_wav_path': wav_path, # Store original path\n",
    "                'speaker_code': speaker_code,\n",
    "                'component': component,\n",
    "                'file_root': file_root,\n",
    "                'transcription': transcription,\n",
    "                'whisper_segments': whisper_segments,\n",
    "                'ort_segments': ort_segments if reference_parsed else [],\n",
    "                'wer_data': wer_data,\n",
    "                'reference_found_path': reference_found_path,\n",
    "                'reference_parsed': reference_parsed,\n",
    "            }\n",
    "            save_results(config['OUTPUT_DIR'], file_base_name, output_data)\n",
    "\n",
    "            # Collect summary info\n",
    "            results_summary.append({\n",
    "                \"processed_wav_file\": file_name, # Name of the file actually processed\n",
    "                \"speaker_code\": speaker_code,\n",
    "                \"component\": component,\n",
    "                \"file_root\": file_root,\n",
    "                \"transcription\": transcription,\n",
    "                \"detected_language\": result.get(\"language\", \"\"),\n",
    "                \"reference_found_path\": reference_found_path,\n",
    "                \"reference_parsed\": reference_parsed,\n",
    "                \"ref_text_preview\": (\" \".join(s['text'] for s in ort_segments)[:80]+\"...\" if reference_parsed else \"\"),\n",
    "                \"num_whisper_segments\": len(whisper_segments),\n",
    "                \"num_ort_segments\": len(ort_segments) if reference_parsed else 0,\n",
    "            })\n",
    "            processed_count += 1\n",
    "\n",
    "        except Exception as loop_error:\n",
    "             print(f\"!! Unexpected Error processing recording at index {index}: {loop_error}\")\n",
    "             print(f\"   Row data: {row.to_dict()}\")\n",
    "             traceback.print_exc() # This one is good, it's already there for the outer loop\n",
    "             skipped_count += 1\n",
    "\n",
    "    # --- Group Summary ---\n",
    "    end_time_group = time.time()\n",
    "    total_time_group = end_time_group - start_time_group\n",
    "    num_processed_successfully = processed_count\n",
    "\n",
    "    print(f\"\\n--- {config['GROUP_NAME']} Processing Summary ---\")\n",
    "    print(f\"Selected {len(recordings_to_process)} recordings for processing.\")\n",
    "    print(f\"Successfully processed: {num_processed_successfully}\")\n",
    "    print(f\"Skipped due to errors/missing files: {skipped_count}\")\n",
    "    print(f\"Total processing time: {total_time_group:.2f} seconds\")\n",
    "    if num_processed_successfully > 0:\n",
    "        print(f\"Avg. time per processed file: {total_time_group / num_processed_successfully:.2f} seconds\")\n",
    "\n",
    "    print(f\"\\n--- {config['GROUP_NAME']} Reference File Summary ---\")\n",
    "    print(f\"Found & Parsed: {ort_found_parsed_count}\")\n",
    "    print(f\"Found but Failed to Parse: {ort_found_not_parsed_count}\")\n",
    "    print(f\"Not Found: {ort_not_found_count}\")\n",
    "    if num_processed_successfully > 0:\n",
    "        parse_success_rate = (ort_found_parsed_count / num_processed_successfully) * 100\n",
    "        print(f\"Parse Success Rate (of processed files): {parse_success_rate:.1f}%\")\n",
    "\n",
    "    # --- Save Group Summary CSV ---\n",
    "    if results_summary:\n",
    "        summary_df = pd.DataFrame(results_summary)\n",
    "        csv_path = os.path.join(config['OUTPUT_DIR'], f\"transcription_summary_{config['GROUP_NAME']}.csv\")\n",
    "        try:\n",
    "            summary_df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "            print(f\"\\nSummary results for {config['GROUP_NAME']} saved to: {csv_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError saving summary CSV for {config['GROUP_NAME']}: {e}\")\n",
    "\n",
    "    # --- Generate Group WER Analysis File ---\n",
    "    generate_wer_analysis_file(results_summary, config['OUTPUT_DIR'], config['GROUP_NAME'])\n",
    "\n",
    "    print(f\"===== Finished Group: {config['GROUP_NAME']} =====\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab492599-2d6c-44ec-af9f-36330c817664",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Environment",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
