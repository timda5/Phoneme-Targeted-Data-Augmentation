{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f285c885-920c-497f-8fa6-c62e872bf815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main processing function 'process_file' REVISED for file-level concatenation AND HYPOTHESIS FILTERING.\n",
      "\n",
      "--- Processing Group: Dutch Speakers ---\n",
      "Input JSON directory: ./../whisper_transcriptions/PureDutchChildren_7_11_Large_3_finetuned\n",
      "Output Ref file (unsorted): ./kaldi_formatted_output_filtered/large3/ref_nl_large3_finetuned_unsorted.txt\n",
      "Output Hyp file (unsorted): ./kaldi_formatted_output_filtered/large3/hyp_nl_large3_finetuned_unsorted.txt\n",
      "Cleared existing output files for Dutch Speakers.\n",
      "Found 78 JSON files for Dutch Speakers. Starting processing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "445e364dfc91493fb0077f0618fe46b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Dutch Speakers:   0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Info: Max reference end time for N000025_fn000049: 460.538s\n",
      "  Info: Max reference end time for N000026_fn000051: 639.572s\n",
      "  Info: Max reference end time for N000027_fn000074: 433.625s\n",
      "  Info: Max reference end time for N000028_fn000060: 401.276s\n",
      "  Info: Max reference end time for N000029_fn000062: 541.946s\n",
      "  Info: Max reference end time for N000030_fn000064: 487.742s\n",
      "  Info: Max reference end time for N000031_fn000066: 561.143s\n",
      "  Info: Max reference end time for N000032_fn000068: 402.875s\n",
      "  Info: Max reference end time for N000033_fn000070: 303.053s\n",
      "  Info: Max reference end time for N000034_fn000073: 381.539s\n",
      "  Info: Max reference end time for N000036_fn000076: 370.815s\n",
      "  Info: Max reference end time for N000037_fn000078: 384.000s\n",
      "  Info: Max reference end time for N000038_fn000080: 394.794s\n",
      "  Info: Max reference end time for N000039_fn000082: 572.632s\n",
      "  Info: Max reference end time for N000040_fn000084: 401.630s\n",
      "  Info: Max reference end time for N000041_fn000087: 395.136s\n",
      "  Info: Max reference end time for N000042_fn000090: 384.922s\n",
      "  Info: Max reference end time for N000043_fn000092: 392.557s\n",
      "  Info: Max reference end time for N000044_fn000094: 581.961s\n",
      "  Info: Max reference end time for N000047_fn000100: 473.127s\n",
      "  Info: Max reference end time for N000048_fn000102: 464.257s\n",
      "  Info: Max reference end time for N000049_fn000104: 373.425s\n",
      "  Info: Max reference end time for N000051_fn000110: 445.568s\n",
      "  Info: Max reference end time for N000052_fn000112: 422.942s\n",
      "  Info: Max reference end time for N000054_fn000108: 650.485s\n",
      "  Info: Max reference end time for N000055_fn000117: 415.809s\n",
      "  Info: Max reference end time for N000056_fn000119: 379.733s\n",
      "  Info: Max reference end time for N000057_fn000121: 284.280s\n",
      "  Info: Max reference end time for N000058_fn000123: 271.399s\n",
      "  Info: Max reference end time for N000059_fn000125: 279.646s\n",
      "  Info: Max reference end time for N000060_fn000127: 320.868s\n",
      "  Info: Max reference end time for N000061_fn000129: 523.484s\n",
      "  Info: Max reference end time for N000062_fn000133: 337.071s\n",
      "  Info: Max reference end time for N000063_fn000135: 304.256s\n",
      "  Info: Max reference end time for N000064_fn000137: 369.384s\n",
      "  Info: Max reference end time for N000066_fn000143: 260.555s\n",
      "  Info: Max reference end time for N000067_fn000145: 482.818s\n",
      "  Info: Max reference end time for N000069_fn000151: 241.803s\n",
      "  Info: Max reference end time for N000070_fn000149: 513.418s\n",
      "  Info: Max reference end time for N000194_fn000494: 579.174s\n",
      "  Info: Max reference end time for N000195_fn000497: 555.974s\n",
      "  Info: Max reference end time for N000197_fn000503: 614.927s\n",
      "  Info: Max reference end time for N000199_fn000510: 497.851s\n",
      "  Info: Max reference end time for N000200_fn000515: 471.995s\n",
      "  Info: Max reference end time for N000201_fn000518: 578.406s\n",
      "  Info: Max reference end time for N000202_fn000521: 508.844s\n",
      "  Info: Max reference end time for N000203_fn000524: 646.785s\n",
      "  Info: Max reference end time for N000206_fn000534: 479.406s\n",
      "  Info: Max reference end time for N000207_fn000537: 462.956s\n",
      "  Info: Max reference end time for N000210_fn000546: 458.388s\n",
      "  Info: Max reference end time for N000211_fn000549: 694.390s\n",
      "  Info: Max reference end time for N000213_fn000555: 448.519s\n",
      "  Info: Max reference end time for N000214_fn000558: 465.125s\n",
      "  Info: Max reference end time for V000072_fv170058: 277.715s\n",
      "  Info: Max reference end time for V000074_fv170061: 331.995s\n",
      "  Info: Max reference end time for V000077_fv170064: 256.987s\n",
      "  Info: Max reference end time for V000086_fv170073: 224.000s\n",
      "  Info: Max reference end time for V000087_fv170074: 419.184s\n",
      "  Info: Max reference end time for V000088_fv170075: 428.315s\n",
      "  Info: Max reference end time for V000089_fv170076: 313.000s\n",
      "  Info: Max reference end time for V000090_fv170077: 417.845s\n",
      "  Info: Max reference end time for V000091_fv170078: 368.000s\n",
      "  Info: Max reference end time for V000092_fv170079: 357.000s\n",
      "  Info: Max reference end time for V000093_fv170080: 267.355s\n",
      "  Info: Max reference end time for V000094_fv170081: 436.241s\n",
      "  Info: Max reference end time for V000096_fv170083: 447.124s\n",
      "  Info: Max reference end time for V000097_fv170084: 392.423s\n",
      "  Info: Max reference end time for V000115_fv170101: 256.282s\n",
      "  Info: Max reference end time for V000121_fv170107: 360.922s\n",
      "  Info: Max reference end time for V000124_fv170110: 394.412s\n",
      "  Info: Max reference end time for V000128_fv170114: 386.392s\n",
      "  Info: Max reference end time for V000129_fv170115: 362.799s\n",
      "  Info: Max reference end time for V000131_fv170120: 278.761s\n",
      "  Info: Max reference end time for V000138_fv170128: 328.667s\n",
      "  Info: Max reference end time for V000142_fv170132: 361.008s\n",
      "  Info: Max reference end time for V000215_fv160205: 326.598s\n",
      "  Info: Max reference end time for V000221_fv170198: 344.390s\n",
      "  Info: Max reference end time for V000222_fv170196: 392.207s\n",
      "Finished processing Dutch Speakers. Total files processed: 78\n",
      "Sorting output files for Dutch Speakers...\n",
      "Created sorted files: ./kaldi_formatted_output_filtered/large3/ref_nl_large3_finetuned.txt and ./kaldi_formatted_output_filtered/large3/hyp_nl_large3_finetuned.txt\n",
      "Removing intermediate unsorted files for Dutch Speakers...\n",
      "------------------------------\n",
      "\n",
      "--- Processing Group: French Speakers ---\n",
      "Input JSON directory: ./../whisper_transcriptions/PureFrenchChildren_7_11_Large_3_finetuned\n",
      "Output Ref file (unsorted): ./kaldi_formatted_output_filtered/large3/ref_fr_large3_finetuned_unsorted.txt\n",
      "Output Hyp file (unsorted): ./kaldi_formatted_output_filtered/large3/hyp_fr_large3_finetuned_unsorted.txt\n",
      "Cleared existing output files for French Speakers.\n",
      "Found 26 JSON files for French Speakers. Starting processing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5325d5b511bb489d82e9d6746ecc3126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing French Speakers:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Info: Max reference end time for V000070_fv170059: 392.825s\n",
      "  Info: Max reference end time for V000112_fv170099: 306.394s\n",
      "  Info: Max reference end time for V000113_fv170098: 307.500s\n",
      "  Info: Max reference end time for V000127_fv170113: 274.950s\n",
      "  Info: Max reference end time for V000130_fv170116: 213.500s\n",
      "  Info: Max reference end time for V000132_fv170121: 430.000s\n",
      "  Info: Max reference end time for V000133_fv170119: 447.951s\n",
      "  Info: Max reference end time for V000134_fv170124: 294.500s\n",
      "  Info: Max reference end time for V000135_fv170125: 270.616s\n",
      "  Info: Max reference end time for V000136_fv170126: 368.500s\n",
      "  Info: Max reference end time for V000137_fv170127: 329.000s\n",
      "  Info: Max reference end time for V000141_fv170131: 357.929s\n",
      "  Info: Max reference end time for V000168_fv170154: 337.860s\n",
      "  Info: Max reference end time for V000173_fv170159: 282.563s\n",
      "  Info: Max reference end time for V000193_fv170174: 329.153s\n",
      "  Info: Max reference end time for V000194_fv170165: 283.500s\n",
      "  Info: Max reference end time for V000195_fv170179: 474.552s\n",
      "  Info: Max reference end time for V000196_fv170192: 490.000s\n",
      "  Info: Max reference end time for V000197_fv170182: 369.891s\n",
      "  Info: Max reference end time for V000198_fv170185: 222.156s\n",
      "  Info: Max reference end time for V000199_fv170180: 384.922s\n",
      "  Info: Max reference end time for V000200_fv160187: 225.125s\n",
      "  Info: Max reference end time for V000201_fv170189: 420.867s\n",
      "  Info: Max reference end time for V000202_fv170187: 369.108s\n",
      "  Info: Max reference end time for V000203_fv170181: 414.898s\n",
      "  Info: Max reference end time for V000204_fv170191: 247.391s\n",
      "Finished processing French Speakers. Total files processed: 26\n",
      "Sorting output files for French Speakers...\n",
      "Created sorted files: ./kaldi_formatted_output_filtered/large3/ref_fr_large3_finetuned.txt and ./kaldi_formatted_output_filtered/large3/hyp_fr_large3_finetuned.txt\n",
      "Removing intermediate unsorted files for French Speakers...\n",
      "------------------------------\n",
      "\n",
      "Script finished.\n",
      "IMPORTANT: Check the final sorted .txt files in 'kaldi_formatted_output_filtered/large3'.\n",
      "Each file should now contain one line per original JSON file processed, with *** removed.\n",
      "Use these final sorted files with compute-wer or Karwi analysis.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from tqdm.notebook import tqdm \n",
    "import numpy as np \n",
    "\n",
    "# --- Normalization and helper functions ---\n",
    "def normalize_jasmin_reference(text):\n",
    "    \"\"\"\n",
    "    Special normalization for JASMIN reference transcripts with annotations.\n",
    "    Handles all special markers and phenomena in the JASMIN corpus.\n",
    "    Includes rules to remove common non-speech annotations.\n",
    "    NOW INCLUDES *** REMOVAL.\n",
    "    \"\"\"\n",
    "    if not text: return \"\"\n",
    "    text = text.upper()\n",
    "    # Remove interviewer labels and metadata-like content\n",
    "    text = re.sub(r'INTERVALTIER|INTERVIEWSTER|GEÏNTERVIEWDE', '', text)\n",
    "    # Remove speech disfluency markers (*A, *U, *F, *S, *Z, etc.)\n",
    "    text = re.sub(r'\\*[A-Z]', '', text)\n",
    "    # Remove hesitation markers\n",
    "    text = re.sub(r'\\b(?:UH|UHM)\\b', '', text)\n",
    "    # Handle repeated words (e.g., \"DE DE\") - keep just one occurrence\n",
    "    text = re.sub(r'\\b(\\w+)(\\s+\\1)+\\b', r'\\1', text)\n",
    "    # Handle stutters (e.g., \"NEDERL NEDERLAND\" or \"NEDER NEDERLAND\")\n",
    "    words = text.split()\n",
    "    i = 0\n",
    "    while i < len(words) - 1:\n",
    "        if len(words[i]) >= 3 and words[i+1].startswith(words[i]) and len(words[i]) < len(words[i+1]):\n",
    "            words.pop(i)\n",
    "        else:\n",
    "            i += 1\n",
    "    text = ' '.join(words)\n",
    "\n",
    "    # --- Added/Enhanced Rules for non-speech ---\n",
    "    text = re.sub(r'\\b(GGG|MMM|XXX|PFF|UHU|AH|JA|NEE|OKÉ)\\b', '', text) # Remove specific annotations/fillers\n",
    "    text = re.sub(r'^TEKST INLEIDING$', '', text) # Remove specific instructions\n",
    "    text = re.sub(r'\\b([A-Z])\\1{2,}\\b', '', text) # Remove 3+ repetitions of same letter like GGG\n",
    "    text = re.sub(r'^[A-Z]$', '', text) # Remove lines containing only a single capital letter (often annotations)\n",
    "    text = re.sub(r'U MAG MU XXX', '', text) # Remove specific complex annotation\n",
    "    text = re.sub(r'CETAIT PAS XXX', '', text) # Remove specific complex annotation\n",
    "    text = re.sub(r'POEH', '', text) # Remove specific filler\n",
    "    text = re.sub(r'\\s*\\*\\*\\*\\s*', ' ', text)\n",
    "\n",
    "    # Remove punctuation except for apostrophes\n",
    "    text = re.sub(r'[^\\w\\s\\']', '', text) # Keep apostrophes within words\n",
    "    # Replace multiple spaces with a single space and strip leading/trailing whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def normalize_hypothesis(text):\n",
    "    \"\"\"\n",
    "    Normalize hypothesis text (Whisper output) for fair comparison.\n",
    "    NOW INCLUDES *** REMOVAL.\n",
    "    \"\"\"\n",
    "    if not text: return \"\"\n",
    "    text = text.upper()\n",
    "    text = re.sub(r'\\s*\\*\\*\\*\\s*', ' ', text)\n",
    "    # Remove punctuation except for apostrophes\n",
    "    text = re.sub(r'[^\\w\\s\\']', '', text)\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def process_file(json_file_path, ref_output_path, hyp_output_path):\n",
    "    \"\"\"\n",
    "    Processes one JSON file to create ONE aligned ref.txt and hyp.txt entry\n",
    "    by concatenating all valid segments within that file.\n",
    "    Applies filtering to the hypothesis based on the reference end time.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading/decoding JSON {json_file_path}: {e}\", file=sys.stderr)\n",
    "        return 0 \n",
    "\n",
    "    base_name = os.path.basename(json_file_path)\n",
    "    file_id_match = re.match(r\"([NV]\\d+)(?:_comp-q)?_(fv\\d+|fn\\d+)\", base_name)\n",
    "    if file_id_match:\n",
    "        file_id = f\"{file_id_match.group(1)}_{file_id_match.group(2)}\"\n",
    "    else:\n",
    "        file_id = os.path.splitext(base_name)[0]\n",
    "        print(f\"Warning: Using fallback file ID for {base_name}: {file_id}\", file=sys.stderr)\n",
    "\n",
    "    # --- Concatenate Reference Text (No change needed here) ---\n",
    "    ort_segments = data.get('ort_segments', [])\n",
    "    all_ref_texts = []\n",
    "    max_ref_time = None # Initialize max reference time\n",
    "    reference_parsed = data.get('reference_parsed', False)\n",
    "\n",
    "    if reference_parsed and ort_segments:\n",
    "        for seg in ort_segments:\n",
    "            ref_text = seg.get('text', '')\n",
    "            if ref_text:\n",
    "                ref_text_norm = normalize_jasmin_reference(ref_text)\n",
    "                if ref_text_norm:\n",
    "                    all_ref_texts.append(ref_text_norm)\n",
    "            # Find the maximum end time from valid segments\n",
    "            try:\n",
    "                end_time = seg.get('end')\n",
    "                if end_time is not None:\n",
    "                    current_end_time = float(end_time)\n",
    "                    if max_ref_time is None or current_end_time > max_ref_time:\n",
    "                        max_ref_time = current_end_time\n",
    "            except (ValueError, TypeError) as time_err:\n",
    "                print(f\"Warning: Invalid end time '{seg.get('end')}' in ORT segment for {file_id}. Skipping for max_ref_time.\", file=sys.stderr)\n",
    "                pass \n",
    "\n",
    "        if max_ref_time is not None:\n",
    "             print(f\"  Info: Max reference end time for {file_id}: {max_ref_time:.3f}s\")\n",
    "        else:\n",
    "             print(f\"  Warning: Could not determine max reference end time for {file_id} despite reference_parsed=True. Using full hypothesis.\")\n",
    "             reference_parsed = False \n",
    "\n",
    "    elif reference_parsed and not ort_segments:\n",
    "         print(f\"  Warning: reference_parsed=True but ort_segments is empty for {file_id}. Using full hypothesis.\")\n",
    "         reference_parsed = False \n",
    "    else:\n",
    "         pass \n",
    "\n",
    "    full_ref_norm = ' '.join(all_ref_texts)\n",
    "\n",
    "    # --- Concatenate Hypothesis Text (WITH FILTERING) ---\n",
    "    whisper_segments = data.get('whisper_segments', [])\n",
    "    all_hyp_texts = []\n",
    "    for seg in whisper_segments:\n",
    "        hyp_text = seg.get('text', '')\n",
    "        if hyp_text:\n",
    "            if reference_parsed and max_ref_time is not None:\n",
    "                try:\n",
    "                    seg_start_time = seg.get('start')\n",
    "                    if seg_start_time is not None and float(seg_start_time) <= max_ref_time:\n",
    "                        hyp_text_norm = normalize_hypothesis(hyp_text)\n",
    "                        if hyp_text_norm:\n",
    "                            all_hyp_texts.append(hyp_text_norm)\n",
    "              \n",
    "                except (ValueError, TypeError) as time_err:\n",
    "                     print(f\"Warning: Invalid start time '{seg.get('start')}' in Whisper segment for {file_id}. Including segment by default.\", file=sys.stderr)\n",
    "        \n",
    "                     hyp_text_norm = normalize_hypothesis(hyp_text)\n",
    "                     if hyp_text_norm:\n",
    "                         all_hyp_texts.append(hyp_text_norm)\n",
    "            else:\n",
    " \n",
    "                hyp_text_norm = normalize_hypothesis(hyp_text)\n",
    "                if hyp_text_norm:\n",
    "                    all_hyp_texts.append(hyp_text_norm)\n",
    "\n",
    "    full_hyp_norm = ' '.join(all_hyp_texts)\n",
    "\n",
    "    # --- Write ONE line per file ---\n",
    "    try:\n",
    "        # Only write if the reference was originally found/parsed, even if empty after normalization\n",
    "        # This ensures ref/hyp files have corresponding lines for WER calculation.\n",
    "        # If reference_parsed is False, we might still want to write the hyp for inspection,\n",
    "        # but it won't be used in standard WER if the ref line is missing/empty.\n",
    "        # Let's write both lines regardless, but WER tools will handle mismatches.\n",
    "        with open(ref_output_path, 'a', encoding='utf-8') as f_ref, \\\n",
    "             open(hyp_output_path, 'a', encoding='utf-8') as f_hyp:\n",
    "            # Write line even if text is empty after normalization/filtering\n",
    "            f_ref.write(f\"{file_id} {full_ref_norm}\\n\")\n",
    "            f_hyp.write(f\"{file_id} {full_hyp_norm}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during writing for {json_file_path}: {e}\", file=sys.stderr)\n",
    "        return 0 # Indicate failure\n",
    "\n",
    "    return 1\n",
    "\n",
    "print(\"Main processing function 'process_file' REVISED for file-level concatenation AND HYPOTHESIS FILTERING.\")\n",
    "\n",
    "# --- Configuration ---\n",
    "notebook_base_dir = \".\"\n",
    "\n",
    "output_folder_name = \"kaldi_formatted_output_filtered/large3\" # Suggest using a new name to avoid confusion\n",
    "output_base_dir = os.path.join(notebook_base_dir, output_folder_name)\n",
    "\n",
    "# --- Group 1: Dutch Speakers ---\n",
    "# Assuming original JSONs are one level up from where the script runs\n",
    "json_input_dir_nl = os.path.join(notebook_base_dir, \"../whisper_transcriptions/PureDutchChildren_7_11_Large_3_finetuned\")\n",
    "ref_output_file_nl_unsorted = os.path.join(output_base_dir, \"ref_nl_large3_finetuned_unsorted.txt\")\n",
    "hyp_output_file_nl_unsorted = os.path.join(output_base_dir, \"hyp_nl_large3_finetuned_unsorted.txt\")\n",
    "\n",
    "# --- Group 2: French Speakers ---\n",
    "json_input_dir_fr = os.path.join(notebook_base_dir, \"../whisper_transcriptions/PureFrenchChildren_7_11_Large_3_finetuned\")\n",
    "ref_output_file_fr_unsorted = os.path.join(output_base_dir, \"ref_fr_large3_finetuned_unsorted.txt\")\n",
    "hyp_output_file_fr_unsorted = os.path.join(output_base_dir, \"hyp_fr_large3_finetuned_unsorted.txt\")\n",
    "\n",
    "# --- Function to run processing for a group (includes sorting) ---\n",
    "def run_group_processing(group_name, json_dir, ref_out_unsorted, hyp_out_unsorted):\n",
    "    print(f\"\\n--- Processing Group: {group_name} ---\")\n",
    "    print(f\"Input JSON directory: {json_dir}\")\n",
    "    print(f\"Output Ref file (unsorted): {ref_out_unsorted}\")\n",
    "    print(f\"Output Hyp file (unsorted): {hyp_out_unsorted}\")\n",
    "\n",
    "    if not os.path.isdir(json_dir):\n",
    "        print(f\"Error: Input directory not found: {json_dir}\", file=sys.stderr)\n",
    "        print(f\"Skipping processing for {group_name}.\")\n",
    "        return\n",
    "\n",
    "    output_dir = os.path.dirname(ref_out_unsorted)\n",
    "    if output_dir and not os.path.exists(output_dir):\n",
    "        try:\n",
    "            os.makedirs(output_dir)\n",
    "            print(f\"Created output directory: {output_dir}\")\n",
    "        except OSError as e:\n",
    "            print(f\"Error creating output directory {output_dir}: {e}\", file=sys.stderr)\n",
    "            return\n",
    "\n",
    "    ref_out_sorted = ref_out_unsorted.replace(\"_unsorted\", \"\")\n",
    "    hyp_out_sorted = hyp_out_unsorted.replace(\"_unsorted\", \"\")\n",
    "    if os.path.exists(ref_out_unsorted): os.remove(ref_out_unsorted)\n",
    "    if os.path.exists(hyp_out_unsorted): os.remove(hyp_out_unsorted)\n",
    "    if os.path.exists(ref_out_sorted): os.remove(ref_out_sorted)\n",
    "    if os.path.exists(hyp_out_sorted): os.remove(hyp_out_sorted)\n",
    "    print(f\"Cleared existing output files for {group_name}.\")\n",
    "\n",
    "    try:\n",
    "        json_files = sorted([f for f in os.listdir(json_dir) if f.endswith(\".json\")])\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Cannot list files in {json_dir}. Check permissions.\", file=sys.stderr)\n",
    "        return\n",
    "    if not json_files:\n",
    "        print(f\"Warning: No JSON files found in {json_dir}\", file=sys.stderr)\n",
    "\n",
    "    print(f\"Found {len(json_files)} JSON files for {group_name}. Starting processing...\")\n",
    "    total_files_processed = 0\n",
    "    for filename in tqdm(json_files, desc=f\"Processing {group_name}\"):\n",
    "        json_path = os.path.join(json_dir, filename)\n",
    "        total_files_processed += process_file(json_path, ref_out_unsorted, hyp_out_unsorted)\n",
    "    print(f\"Finished processing {group_name}. Total files processed: {total_files_processed}\")\n",
    "\n",
    "    print(f\"Sorting output files for {group_name}...\")\n",
    "    ref_out_sorted = ref_out_unsorted.replace(\"_unsorted\", \"\")\n",
    "    hyp_out_sorted = hyp_out_unsorted.replace(\"_unsorted\", \"\")\n",
    "\n",
    "    try:\n",
    "        sort_command = \"LC_ALL=C sort\" \n",
    "        if sys.platform == \"win32\":\n",
    "             sort_command = \"sort\"\n",
    "\n",
    "  \n",
    "        if os.path.exists(ref_out_unsorted):\n",
    "             os.system(f\"{sort_command} -k1,1 {ref_out_unsorted} > {ref_out_sorted}\")\n",
    "        else:\n",
    "      \n",
    "             open(ref_out_sorted, 'a').close()\n",
    "\n",
    "        if os.path.exists(hyp_out_unsorted):\n",
    "             os.system(f\"{sort_command} -k1,1 {hyp_out_unsorted} > {hyp_out_sorted}\")\n",
    "        else:\n",
    "             open(hyp_out_sorted, 'a').close()\n",
    "\n",
    "        print(f\"Created sorted files: {ref_out_sorted} and {hyp_out_sorted}\")\n",
    "\n",
    "        # Remove intermediate unsorted files if they exist\n",
    "        print(f\"Removing intermediate unsorted files for {group_name}...\")\n",
    "        if os.path.exists(ref_out_unsorted): os.remove(ref_out_unsorted)\n",
    "        if os.path.exists(hyp_out_unsorted): os.remove(hyp_out_unsorted)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error sorting or removing files for {group_name}: {e}\", file=sys.stderr)\n",
    "        print(\"Please check the files manually.\")\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# --- Execute for all groups ---\n",
    "run_group_processing(\"Dutch Speakers\", json_input_dir_nl, ref_output_file_nl_unsorted, hyp_output_file_nl_unsorted)\n",
    "run_group_processing(\"French Speakers\", json_input_dir_fr, ref_output_file_fr_unsorted, hyp_output_file_fr_unsorted)\n",
    "\n",
    "print(\"\\nScript finished.\")\n",
    "print(f\"IMPORTANT: Check the final sorted .txt files in '{output_folder_name}'.\")\n",
    "print(\"Each file should now contain one line per original JSON file processed, with *** removed.\")\n",
    "print(\"Use these final sorted files with compute-wer or Karwi analysis.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7596b6b1-86f0-4079-9c9c-1666e38b5410",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Environment",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
